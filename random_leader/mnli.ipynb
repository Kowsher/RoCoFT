{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd48bc06-2c45-4226-b18a-802795f18cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets  = load_dataset(\"glue\", 'mnli')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af2d704-b278-45aa-88eb-d442a299e322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f7b9f8-fb0c-472c-984c-4e7403b0485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "col_to_delete = ['question1','sentence2']\n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocessing_function, batched=True)\n",
    "\n",
    "\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3e1e1e-a176-476d-a09b-ffef8c304fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49a4e8f-0039-410e-ab82-5dfb55617f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leader\n",
    "\n",
    "leader.PEFT(model, method='random', rank=0.1) \n",
    "#targets=['key', 'value', 'dense', 'query'])\n",
    "# method = 'row', 'column', 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20873d5b-6578-4315-a6cd-cdfb5ffffb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5b54f3-7558-4f8f-bd0d-053e50495741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dir',\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.00,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=500,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2227cdf-7c5d-4dfd-894b-a2e64ebc0366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49088' max='49088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49088/49088 2:19:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.840500</td>\n",
       "      <td>0.584945</td>\n",
       "      <td>0.779995</td>\n",
       "      <td>0.773005</td>\n",
       "      <td>0.771279</td>\n",
       "      <td>0.771268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.533628</td>\n",
       "      <td>0.806704</td>\n",
       "      <td>0.806237</td>\n",
       "      <td>0.805943</td>\n",
       "      <td>0.806419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.575600</td>\n",
       "      <td>0.563280</td>\n",
       "      <td>0.806105</td>\n",
       "      <td>0.799548</td>\n",
       "      <td>0.798258</td>\n",
       "      <td>0.800204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.543700</td>\n",
       "      <td>0.516414</td>\n",
       "      <td>0.813164</td>\n",
       "      <td>0.812693</td>\n",
       "      <td>0.812469</td>\n",
       "      <td>0.812939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.498584</td>\n",
       "      <td>0.816738</td>\n",
       "      <td>0.816641</td>\n",
       "      <td>0.815756</td>\n",
       "      <td>0.816607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>0.456787</td>\n",
       "      <td>0.826173</td>\n",
       "      <td>0.826266</td>\n",
       "      <td>0.825606</td>\n",
       "      <td>0.826388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>0.461485</td>\n",
       "      <td>0.828184</td>\n",
       "      <td>0.827569</td>\n",
       "      <td>0.827685</td>\n",
       "      <td>0.828222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.526400</td>\n",
       "      <td>0.459230</td>\n",
       "      <td>0.823977</td>\n",
       "      <td>0.822464</td>\n",
       "      <td>0.821258</td>\n",
       "      <td>0.823230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>0.439741</td>\n",
       "      <td>0.835887</td>\n",
       "      <td>0.835831</td>\n",
       "      <td>0.835768</td>\n",
       "      <td>0.837086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.513300</td>\n",
       "      <td>0.463325</td>\n",
       "      <td>0.830008</td>\n",
       "      <td>0.830004</td>\n",
       "      <td>0.829373</td>\n",
       "      <td>0.830362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>0.438280</td>\n",
       "      <td>0.834333</td>\n",
       "      <td>0.830036</td>\n",
       "      <td>0.830800</td>\n",
       "      <td>0.832705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.487200</td>\n",
       "      <td>0.432365</td>\n",
       "      <td>0.837775</td>\n",
       "      <td>0.836216</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.837799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.426572</td>\n",
       "      <td>0.838390</td>\n",
       "      <td>0.838403</td>\n",
       "      <td>0.838202</td>\n",
       "      <td>0.838818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.423552</td>\n",
       "      <td>0.839032</td>\n",
       "      <td>0.836371</td>\n",
       "      <td>0.836072</td>\n",
       "      <td>0.838818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.510773</td>\n",
       "      <td>0.823516</td>\n",
       "      <td>0.819003</td>\n",
       "      <td>0.817380</td>\n",
       "      <td>0.820071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.471800</td>\n",
       "      <td>0.444295</td>\n",
       "      <td>0.835260</td>\n",
       "      <td>0.835304</td>\n",
       "      <td>0.834572</td>\n",
       "      <td>0.835252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.470300</td>\n",
       "      <td>0.437578</td>\n",
       "      <td>0.838873</td>\n",
       "      <td>0.839086</td>\n",
       "      <td>0.838835</td>\n",
       "      <td>0.840041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.472656</td>\n",
       "      <td>0.834119</td>\n",
       "      <td>0.830014</td>\n",
       "      <td>0.828517</td>\n",
       "      <td>0.828222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.455161</td>\n",
       "      <td>0.834513</td>\n",
       "      <td>0.834143</td>\n",
       "      <td>0.833158</td>\n",
       "      <td>0.834233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.429245</td>\n",
       "      <td>0.844827</td>\n",
       "      <td>0.843095</td>\n",
       "      <td>0.843288</td>\n",
       "      <td>0.845237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.429179</td>\n",
       "      <td>0.840493</td>\n",
       "      <td>0.839187</td>\n",
       "      <td>0.838471</td>\n",
       "      <td>0.840550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>0.419235</td>\n",
       "      <td>0.845142</td>\n",
       "      <td>0.845158</td>\n",
       "      <td>0.845149</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.412076</td>\n",
       "      <td>0.842580</td>\n",
       "      <td>0.841140</td>\n",
       "      <td>0.841551</td>\n",
       "      <td>0.842894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>0.436767</td>\n",
       "      <td>0.845972</td>\n",
       "      <td>0.846311</td>\n",
       "      <td>0.845974</td>\n",
       "      <td>0.846663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.459600</td>\n",
       "      <td>0.413996</td>\n",
       "      <td>0.844186</td>\n",
       "      <td>0.844324</td>\n",
       "      <td>0.844115</td>\n",
       "      <td>0.845441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.411841</td>\n",
       "      <td>0.846950</td>\n",
       "      <td>0.846665</td>\n",
       "      <td>0.846780</td>\n",
       "      <td>0.847886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.411852</td>\n",
       "      <td>0.845268</td>\n",
       "      <td>0.845551</td>\n",
       "      <td>0.845075</td>\n",
       "      <td>0.846358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.406516</td>\n",
       "      <td>0.850182</td>\n",
       "      <td>0.848427</td>\n",
       "      <td>0.847633</td>\n",
       "      <td>0.847478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.405690</td>\n",
       "      <td>0.844182</td>\n",
       "      <td>0.839960</td>\n",
       "      <td>0.840456</td>\n",
       "      <td>0.840754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.398855</td>\n",
       "      <td>0.852436</td>\n",
       "      <td>0.852351</td>\n",
       "      <td>0.852334</td>\n",
       "      <td>0.852980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.440200</td>\n",
       "      <td>0.389926</td>\n",
       "      <td>0.852705</td>\n",
       "      <td>0.852210</td>\n",
       "      <td>0.852274</td>\n",
       "      <td>0.852776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>0.386307</td>\n",
       "      <td>0.855594</td>\n",
       "      <td>0.855988</td>\n",
       "      <td>0.855441</td>\n",
       "      <td>0.856037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.402625</td>\n",
       "      <td>0.851997</td>\n",
       "      <td>0.852446</td>\n",
       "      <td>0.851722</td>\n",
       "      <td>0.852674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.391681</td>\n",
       "      <td>0.853561</td>\n",
       "      <td>0.853745</td>\n",
       "      <td>0.853027</td>\n",
       "      <td>0.853693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.432872</td>\n",
       "      <td>0.850459</td>\n",
       "      <td>0.849601</td>\n",
       "      <td>0.849175</td>\n",
       "      <td>0.849210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.388948</td>\n",
       "      <td>0.856638</td>\n",
       "      <td>0.856824</td>\n",
       "      <td>0.856702</td>\n",
       "      <td>0.857463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.388845</td>\n",
       "      <td>0.852804</td>\n",
       "      <td>0.852142</td>\n",
       "      <td>0.851703</td>\n",
       "      <td>0.853591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.387726</td>\n",
       "      <td>0.857410</td>\n",
       "      <td>0.856843</td>\n",
       "      <td>0.857022</td>\n",
       "      <td>0.858176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.428500</td>\n",
       "      <td>0.403843</td>\n",
       "      <td>0.852298</td>\n",
       "      <td>0.852304</td>\n",
       "      <td>0.851579</td>\n",
       "      <td>0.852165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.393961</td>\n",
       "      <td>0.858632</td>\n",
       "      <td>0.858793</td>\n",
       "      <td>0.858370</td>\n",
       "      <td>0.859705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.433200</td>\n",
       "      <td>0.417286</td>\n",
       "      <td>0.850999</td>\n",
       "      <td>0.847160</td>\n",
       "      <td>0.845739</td>\n",
       "      <td>0.845339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.428500</td>\n",
       "      <td>0.381063</td>\n",
       "      <td>0.857436</td>\n",
       "      <td>0.857432</td>\n",
       "      <td>0.857434</td>\n",
       "      <td>0.858380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.435600</td>\n",
       "      <td>0.383195</td>\n",
       "      <td>0.854174</td>\n",
       "      <td>0.854295</td>\n",
       "      <td>0.853986</td>\n",
       "      <td>0.854610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.424400</td>\n",
       "      <td>0.374641</td>\n",
       "      <td>0.858385</td>\n",
       "      <td>0.858144</td>\n",
       "      <td>0.858202</td>\n",
       "      <td>0.858991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.391848</td>\n",
       "      <td>0.861312</td>\n",
       "      <td>0.860779</td>\n",
       "      <td>0.860500</td>\n",
       "      <td>0.860723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>0.378167</td>\n",
       "      <td>0.857970</td>\n",
       "      <td>0.857475</td>\n",
       "      <td>0.857239</td>\n",
       "      <td>0.857463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.377767</td>\n",
       "      <td>0.860688</td>\n",
       "      <td>0.860554</td>\n",
       "      <td>0.860203</td>\n",
       "      <td>0.860621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.407600</td>\n",
       "      <td>0.386550</td>\n",
       "      <td>0.860804</td>\n",
       "      <td>0.860339</td>\n",
       "      <td>0.860057</td>\n",
       "      <td>0.860316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.410100</td>\n",
       "      <td>0.381680</td>\n",
       "      <td>0.861239</td>\n",
       "      <td>0.861381</td>\n",
       "      <td>0.861078</td>\n",
       "      <td>0.861640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.335200</td>\n",
       "      <td>0.426640</td>\n",
       "      <td>0.857944</td>\n",
       "      <td>0.856844</td>\n",
       "      <td>0.855866</td>\n",
       "      <td>0.857565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.395511</td>\n",
       "      <td>0.862446</td>\n",
       "      <td>0.862031</td>\n",
       "      <td>0.861869</td>\n",
       "      <td>0.862150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>0.409900</td>\n",
       "      <td>0.859562</td>\n",
       "      <td>0.858399</td>\n",
       "      <td>0.857629</td>\n",
       "      <td>0.857565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.391743</td>\n",
       "      <td>0.862630</td>\n",
       "      <td>0.863059</td>\n",
       "      <td>0.862638</td>\n",
       "      <td>0.863474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.339800</td>\n",
       "      <td>0.403507</td>\n",
       "      <td>0.864598</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.864554</td>\n",
       "      <td>0.865206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.385046</td>\n",
       "      <td>0.867640</td>\n",
       "      <td>0.867830</td>\n",
       "      <td>0.867589</td>\n",
       "      <td>0.868161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.394876</td>\n",
       "      <td>0.864833</td>\n",
       "      <td>0.864646</td>\n",
       "      <td>0.864443</td>\n",
       "      <td>0.864901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.344200</td>\n",
       "      <td>0.392452</td>\n",
       "      <td>0.864609</td>\n",
       "      <td>0.864511</td>\n",
       "      <td>0.863916</td>\n",
       "      <td>0.864289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.376686</td>\n",
       "      <td>0.865209</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.865308</td>\n",
       "      <td>0.866123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.385868</td>\n",
       "      <td>0.861950</td>\n",
       "      <td>0.861986</td>\n",
       "      <td>0.861466</td>\n",
       "      <td>0.861946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.397371</td>\n",
       "      <td>0.863461</td>\n",
       "      <td>0.863888</td>\n",
       "      <td>0.863265</td>\n",
       "      <td>0.864187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.392770</td>\n",
       "      <td>0.865798</td>\n",
       "      <td>0.865168</td>\n",
       "      <td>0.865131</td>\n",
       "      <td>0.865410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.373834</td>\n",
       "      <td>0.868480</td>\n",
       "      <td>0.868372</td>\n",
       "      <td>0.868412</td>\n",
       "      <td>0.869485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.407857</td>\n",
       "      <td>0.861272</td>\n",
       "      <td>0.861471</td>\n",
       "      <td>0.860766</td>\n",
       "      <td>0.861437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.384452</td>\n",
       "      <td>0.864559</td>\n",
       "      <td>0.864843</td>\n",
       "      <td>0.864193</td>\n",
       "      <td>0.864799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.323200</td>\n",
       "      <td>0.378568</td>\n",
       "      <td>0.866471</td>\n",
       "      <td>0.865726</td>\n",
       "      <td>0.865428</td>\n",
       "      <td>0.865512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>0.386863</td>\n",
       "      <td>0.865742</td>\n",
       "      <td>0.866008</td>\n",
       "      <td>0.865503</td>\n",
       "      <td>0.866021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.330800</td>\n",
       "      <td>0.379441</td>\n",
       "      <td>0.869158</td>\n",
       "      <td>0.869656</td>\n",
       "      <td>0.869105</td>\n",
       "      <td>0.870097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.371624</td>\n",
       "      <td>0.867090</td>\n",
       "      <td>0.867377</td>\n",
       "      <td>0.867108</td>\n",
       "      <td>0.867753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>0.395186</td>\n",
       "      <td>0.863500</td>\n",
       "      <td>0.863774</td>\n",
       "      <td>0.863225</td>\n",
       "      <td>0.863882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.381306</td>\n",
       "      <td>0.868049</td>\n",
       "      <td>0.868295</td>\n",
       "      <td>0.867877</td>\n",
       "      <td>0.868365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.383238</td>\n",
       "      <td>0.867579</td>\n",
       "      <td>0.867434</td>\n",
       "      <td>0.867049</td>\n",
       "      <td>0.867346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.317600</td>\n",
       "      <td>0.372180</td>\n",
       "      <td>0.866963</td>\n",
       "      <td>0.867012</td>\n",
       "      <td>0.866756</td>\n",
       "      <td>0.867244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.321100</td>\n",
       "      <td>0.383673</td>\n",
       "      <td>0.868179</td>\n",
       "      <td>0.868583</td>\n",
       "      <td>0.868130</td>\n",
       "      <td>0.868772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.399980</td>\n",
       "      <td>0.866968</td>\n",
       "      <td>0.866853</td>\n",
       "      <td>0.866419</td>\n",
       "      <td>0.866735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.382095</td>\n",
       "      <td>0.867021</td>\n",
       "      <td>0.867153</td>\n",
       "      <td>0.866812</td>\n",
       "      <td>0.867244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.367305</td>\n",
       "      <td>0.868735</td>\n",
       "      <td>0.868883</td>\n",
       "      <td>0.868611</td>\n",
       "      <td>0.869078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.313500</td>\n",
       "      <td>0.376261</td>\n",
       "      <td>0.869492</td>\n",
       "      <td>0.869873</td>\n",
       "      <td>0.869625</td>\n",
       "      <td>0.870402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.376496</td>\n",
       "      <td>0.868459</td>\n",
       "      <td>0.868295</td>\n",
       "      <td>0.868127</td>\n",
       "      <td>0.868467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.316700</td>\n",
       "      <td>0.369411</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.868089</td>\n",
       "      <td>0.867747</td>\n",
       "      <td>0.868467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.375799</td>\n",
       "      <td>0.869768</td>\n",
       "      <td>0.870003</td>\n",
       "      <td>0.869747</td>\n",
       "      <td>0.870301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.383029</td>\n",
       "      <td>0.870031</td>\n",
       "      <td>0.870319</td>\n",
       "      <td>0.869954</td>\n",
       "      <td>0.870504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.379511</td>\n",
       "      <td>0.870672</td>\n",
       "      <td>0.870995</td>\n",
       "      <td>0.870779</td>\n",
       "      <td>0.871523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.376418</td>\n",
       "      <td>0.870794</td>\n",
       "      <td>0.871003</td>\n",
       "      <td>0.870763</td>\n",
       "      <td>0.871319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.377759</td>\n",
       "      <td>0.869628</td>\n",
       "      <td>0.870053</td>\n",
       "      <td>0.869704</td>\n",
       "      <td>0.870402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.375391</td>\n",
       "      <td>0.870595</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>0.870437</td>\n",
       "      <td>0.870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.299100</td>\n",
       "      <td>0.376844</td>\n",
       "      <td>0.870602</td>\n",
       "      <td>0.870882</td>\n",
       "      <td>0.870551</td>\n",
       "      <td>0.871116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.367520</td>\n",
       "      <td>0.871727</td>\n",
       "      <td>0.872044</td>\n",
       "      <td>0.871744</td>\n",
       "      <td>0.872338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>0.372341</td>\n",
       "      <td>0.870368</td>\n",
       "      <td>0.870723</td>\n",
       "      <td>0.870316</td>\n",
       "      <td>0.870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.321900</td>\n",
       "      <td>0.370079</td>\n",
       "      <td>0.870260</td>\n",
       "      <td>0.870577</td>\n",
       "      <td>0.870214</td>\n",
       "      <td>0.870810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.311500</td>\n",
       "      <td>0.370364</td>\n",
       "      <td>0.871038</td>\n",
       "      <td>0.871435</td>\n",
       "      <td>0.871062</td>\n",
       "      <td>0.871727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.304600</td>\n",
       "      <td>0.374383</td>\n",
       "      <td>0.869844</td>\n",
       "      <td>0.870251</td>\n",
       "      <td>0.869836</td>\n",
       "      <td>0.870504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.372514</td>\n",
       "      <td>0.870902</td>\n",
       "      <td>0.871284</td>\n",
       "      <td>0.870936</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>0.373257</td>\n",
       "      <td>0.870373</td>\n",
       "      <td>0.870691</td>\n",
       "      <td>0.870321</td>\n",
       "      <td>0.870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.301200</td>\n",
       "      <td>0.372893</td>\n",
       "      <td>0.870657</td>\n",
       "      <td>0.871009</td>\n",
       "      <td>0.870666</td>\n",
       "      <td>0.871319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.309400</td>\n",
       "      <td>0.372911</td>\n",
       "      <td>0.870538</td>\n",
       "      <td>0.870843</td>\n",
       "      <td>0.870509</td>\n",
       "      <td>0.871116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.372910</td>\n",
       "      <td>0.870160</td>\n",
       "      <td>0.870439</td>\n",
       "      <td>0.870118</td>\n",
       "      <td>0.870708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.302500</td>\n",
       "      <td>0.373010</td>\n",
       "      <td>0.870059</td>\n",
       "      <td>0.870340</td>\n",
       "      <td>0.870013</td>\n",
       "      <td>0.870606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.305800</td>\n",
       "      <td>0.372999</td>\n",
       "      <td>0.870059</td>\n",
       "      <td>0.870340</td>\n",
       "      <td>0.870013</td>\n",
       "      <td>0.870606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49088, training_loss=0.39712670589208293, metrics={'train_runtime': 8374.596, 'train_samples_per_second': 93.784, 'train_steps_per_second': 5.862, 'total_flos': 3.264820232568552e+16, 'train_loss': 0.39712670589208293, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009aa81-a7ce-43f2-894a-e7d336c3461b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
