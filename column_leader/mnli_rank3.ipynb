{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd48bc06-2c45-4226-b18a-802795f18cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets  = load_dataset(\"glue\", 'mnli')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af2d704-b278-45aa-88eb-d442a299e322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f7b9f8-fb0c-472c-984c-4e7403b0485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "col_to_delete = ['question1','sentence2']\n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocessing_function, batched=True)\n",
    "\n",
    "\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3e1e1e-a176-476d-a09b-ffef8c304fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49a4e8f-0039-410e-ab82-5dfb55617f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leader\n",
    "\n",
    "leader.PEFT(model, method='column', rank=3) \n",
    "#targets=['key', 'value', 'dense', 'query'])\n",
    "# method = 'row', 'column', 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20873d5b-6578-4315-a6cd-cdfb5ffffb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5b54f3-7558-4f8f-bd0d-053e50495741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dir',\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.00,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=500,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2227cdf-7c5d-4dfd-894b-a2e64ebc0366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49088' max='49088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49088/49088 1:04:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.960200</td>\n",
       "      <td>0.659120</td>\n",
       "      <td>0.740412</td>\n",
       "      <td>0.737455</td>\n",
       "      <td>0.737839</td>\n",
       "      <td>0.738054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.577163</td>\n",
       "      <td>0.773792</td>\n",
       "      <td>0.771571</td>\n",
       "      <td>0.771752</td>\n",
       "      <td>0.771982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.640100</td>\n",
       "      <td>0.614223</td>\n",
       "      <td>0.778942</td>\n",
       "      <td>0.770070</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.772084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.616100</td>\n",
       "      <td>0.556459</td>\n",
       "      <td>0.797790</td>\n",
       "      <td>0.791770</td>\n",
       "      <td>0.792186</td>\n",
       "      <td>0.793072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>0.514135</td>\n",
       "      <td>0.799892</td>\n",
       "      <td>0.800078</td>\n",
       "      <td>0.799795</td>\n",
       "      <td>0.801630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.588800</td>\n",
       "      <td>0.505104</td>\n",
       "      <td>0.803233</td>\n",
       "      <td>0.803458</td>\n",
       "      <td>0.803158</td>\n",
       "      <td>0.803974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.562300</td>\n",
       "      <td>0.517198</td>\n",
       "      <td>0.805264</td>\n",
       "      <td>0.804240</td>\n",
       "      <td>0.804123</td>\n",
       "      <td>0.806521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.578300</td>\n",
       "      <td>0.512528</td>\n",
       "      <td>0.805699</td>\n",
       "      <td>0.804809</td>\n",
       "      <td>0.804648</td>\n",
       "      <td>0.804992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.478304</td>\n",
       "      <td>0.813485</td>\n",
       "      <td>0.813280</td>\n",
       "      <td>0.813244</td>\n",
       "      <td>0.814060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.572400</td>\n",
       "      <td>0.491202</td>\n",
       "      <td>0.814750</td>\n",
       "      <td>0.812565</td>\n",
       "      <td>0.811717</td>\n",
       "      <td>0.811615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.474599</td>\n",
       "      <td>0.812142</td>\n",
       "      <td>0.808454</td>\n",
       "      <td>0.809060</td>\n",
       "      <td>0.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.528400</td>\n",
       "      <td>0.453005</td>\n",
       "      <td>0.824789</td>\n",
       "      <td>0.824249</td>\n",
       "      <td>0.824214</td>\n",
       "      <td>0.826184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.546100</td>\n",
       "      <td>0.460763</td>\n",
       "      <td>0.820789</td>\n",
       "      <td>0.820945</td>\n",
       "      <td>0.820617</td>\n",
       "      <td>0.821803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.525300</td>\n",
       "      <td>0.464161</td>\n",
       "      <td>0.819718</td>\n",
       "      <td>0.818880</td>\n",
       "      <td>0.818486</td>\n",
       "      <td>0.820785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.515800</td>\n",
       "      <td>0.519494</td>\n",
       "      <td>0.808244</td>\n",
       "      <td>0.806322</td>\n",
       "      <td>0.805137</td>\n",
       "      <td>0.807539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.463277</td>\n",
       "      <td>0.823192</td>\n",
       "      <td>0.820018</td>\n",
       "      <td>0.819886</td>\n",
       "      <td>0.819562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.519700</td>\n",
       "      <td>0.455823</td>\n",
       "      <td>0.824473</td>\n",
       "      <td>0.823914</td>\n",
       "      <td>0.824021</td>\n",
       "      <td>0.824860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.532900</td>\n",
       "      <td>0.475453</td>\n",
       "      <td>0.819763</td>\n",
       "      <td>0.817956</td>\n",
       "      <td>0.817017</td>\n",
       "      <td>0.817117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.514900</td>\n",
       "      <td>0.458298</td>\n",
       "      <td>0.826133</td>\n",
       "      <td>0.826198</td>\n",
       "      <td>0.825823</td>\n",
       "      <td>0.826490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.447992</td>\n",
       "      <td>0.828314</td>\n",
       "      <td>0.828484</td>\n",
       "      <td>0.828359</td>\n",
       "      <td>0.829547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.452535</td>\n",
       "      <td>0.823764</td>\n",
       "      <td>0.823836</td>\n",
       "      <td>0.823293</td>\n",
       "      <td>0.824452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.453395</td>\n",
       "      <td>0.827578</td>\n",
       "      <td>0.824959</td>\n",
       "      <td>0.825491</td>\n",
       "      <td>0.826592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.445978</td>\n",
       "      <td>0.827213</td>\n",
       "      <td>0.826430</td>\n",
       "      <td>0.826517</td>\n",
       "      <td>0.828426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.472776</td>\n",
       "      <td>0.827023</td>\n",
       "      <td>0.824640</td>\n",
       "      <td>0.823796</td>\n",
       "      <td>0.826592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.452896</td>\n",
       "      <td>0.823606</td>\n",
       "      <td>0.823384</td>\n",
       "      <td>0.823421</td>\n",
       "      <td>0.824962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.507900</td>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.831229</td>\n",
       "      <td>0.829968</td>\n",
       "      <td>0.830293</td>\n",
       "      <td>0.831890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.485200</td>\n",
       "      <td>0.445213</td>\n",
       "      <td>0.825877</td>\n",
       "      <td>0.826003</td>\n",
       "      <td>0.825529</td>\n",
       "      <td>0.826694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.459355</td>\n",
       "      <td>0.824945</td>\n",
       "      <td>0.822772</td>\n",
       "      <td>0.821683</td>\n",
       "      <td>0.821905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.437131</td>\n",
       "      <td>0.833681</td>\n",
       "      <td>0.828429</td>\n",
       "      <td>0.828710</td>\n",
       "      <td>0.829139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>0.432615</td>\n",
       "      <td>0.835863</td>\n",
       "      <td>0.833665</td>\n",
       "      <td>0.834124</td>\n",
       "      <td>0.834845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>0.431815</td>\n",
       "      <td>0.833436</td>\n",
       "      <td>0.833495</td>\n",
       "      <td>0.833027</td>\n",
       "      <td>0.834335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.503100</td>\n",
       "      <td>0.433873</td>\n",
       "      <td>0.835185</td>\n",
       "      <td>0.834633</td>\n",
       "      <td>0.834491</td>\n",
       "      <td>0.834845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.439345</td>\n",
       "      <td>0.830991</td>\n",
       "      <td>0.830771</td>\n",
       "      <td>0.830142</td>\n",
       "      <td>0.831584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.432139</td>\n",
       "      <td>0.833482</td>\n",
       "      <td>0.833195</td>\n",
       "      <td>0.832548</td>\n",
       "      <td>0.833724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.487800</td>\n",
       "      <td>0.446443</td>\n",
       "      <td>0.838784</td>\n",
       "      <td>0.837126</td>\n",
       "      <td>0.837172</td>\n",
       "      <td>0.837392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.509200</td>\n",
       "      <td>0.431445</td>\n",
       "      <td>0.833316</td>\n",
       "      <td>0.833058</td>\n",
       "      <td>0.832654</td>\n",
       "      <td>0.834437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>0.428644</td>\n",
       "      <td>0.833679</td>\n",
       "      <td>0.833253</td>\n",
       "      <td>0.832908</td>\n",
       "      <td>0.834947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.424004</td>\n",
       "      <td>0.836230</td>\n",
       "      <td>0.835765</td>\n",
       "      <td>0.835923</td>\n",
       "      <td>0.837290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.481500</td>\n",
       "      <td>0.427309</td>\n",
       "      <td>0.835680</td>\n",
       "      <td>0.835728</td>\n",
       "      <td>0.835264</td>\n",
       "      <td>0.835965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.440702</td>\n",
       "      <td>0.834672</td>\n",
       "      <td>0.833302</td>\n",
       "      <td>0.832660</td>\n",
       "      <td>0.835048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.483300</td>\n",
       "      <td>0.443851</td>\n",
       "      <td>0.833922</td>\n",
       "      <td>0.831139</td>\n",
       "      <td>0.829735</td>\n",
       "      <td>0.829954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0.435196</td>\n",
       "      <td>0.832427</td>\n",
       "      <td>0.832001</td>\n",
       "      <td>0.831807</td>\n",
       "      <td>0.833724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.430199</td>\n",
       "      <td>0.834545</td>\n",
       "      <td>0.834616</td>\n",
       "      <td>0.834225</td>\n",
       "      <td>0.835762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.412374</td>\n",
       "      <td>0.840866</td>\n",
       "      <td>0.840772</td>\n",
       "      <td>0.840769</td>\n",
       "      <td>0.841671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.475600</td>\n",
       "      <td>0.423410</td>\n",
       "      <td>0.836597</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>0.836644</td>\n",
       "      <td>0.838003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.419621</td>\n",
       "      <td>0.839190</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.839156</td>\n",
       "      <td>0.840041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.464700</td>\n",
       "      <td>0.422600</td>\n",
       "      <td>0.839514</td>\n",
       "      <td>0.839227</td>\n",
       "      <td>0.838526</td>\n",
       "      <td>0.839226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.432407</td>\n",
       "      <td>0.837745</td>\n",
       "      <td>0.837337</td>\n",
       "      <td>0.837278</td>\n",
       "      <td>0.837901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.414666</td>\n",
       "      <td>0.843677</td>\n",
       "      <td>0.843460</td>\n",
       "      <td>0.843310</td>\n",
       "      <td>0.843810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.450100</td>\n",
       "      <td>0.448104</td>\n",
       "      <td>0.834640</td>\n",
       "      <td>0.831940</td>\n",
       "      <td>0.830875</td>\n",
       "      <td>0.833316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.420379</td>\n",
       "      <td>0.842130</td>\n",
       "      <td>0.842328</td>\n",
       "      <td>0.841805</td>\n",
       "      <td>0.842588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>0.433176</td>\n",
       "      <td>0.837430</td>\n",
       "      <td>0.835808</td>\n",
       "      <td>0.835230</td>\n",
       "      <td>0.835150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.410197</td>\n",
       "      <td>0.844585</td>\n",
       "      <td>0.844692</td>\n",
       "      <td>0.844345</td>\n",
       "      <td>0.845848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.441300</td>\n",
       "      <td>0.417542</td>\n",
       "      <td>0.845094</td>\n",
       "      <td>0.845318</td>\n",
       "      <td>0.845012</td>\n",
       "      <td>0.845746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>0.416980</td>\n",
       "      <td>0.846995</td>\n",
       "      <td>0.846486</td>\n",
       "      <td>0.846410</td>\n",
       "      <td>0.846867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.445100</td>\n",
       "      <td>0.419748</td>\n",
       "      <td>0.841733</td>\n",
       "      <td>0.841269</td>\n",
       "      <td>0.841140</td>\n",
       "      <td>0.841569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>0.418358</td>\n",
       "      <td>0.843025</td>\n",
       "      <td>0.842735</td>\n",
       "      <td>0.842056</td>\n",
       "      <td>0.842486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>0.405500</td>\n",
       "      <td>0.844809</td>\n",
       "      <td>0.844730</td>\n",
       "      <td>0.844374</td>\n",
       "      <td>0.845950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.394293</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.850773</td>\n",
       "      <td>0.850974</td>\n",
       "      <td>0.852267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.434307</td>\n",
       "      <td>0.839145</td>\n",
       "      <td>0.838670</td>\n",
       "      <td>0.837861</td>\n",
       "      <td>0.838920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>0.406422</td>\n",
       "      <td>0.848517</td>\n",
       "      <td>0.848376</td>\n",
       "      <td>0.848439</td>\n",
       "      <td>0.849516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.436500</td>\n",
       "      <td>0.403664</td>\n",
       "      <td>0.849801</td>\n",
       "      <td>0.849545</td>\n",
       "      <td>0.849645</td>\n",
       "      <td>0.850535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.412913</td>\n",
       "      <td>0.842454</td>\n",
       "      <td>0.842011</td>\n",
       "      <td>0.841402</td>\n",
       "      <td>0.841773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>0.412940</td>\n",
       "      <td>0.844161</td>\n",
       "      <td>0.844148</td>\n",
       "      <td>0.843423</td>\n",
       "      <td>0.844320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.434400</td>\n",
       "      <td>0.404665</td>\n",
       "      <td>0.847042</td>\n",
       "      <td>0.846376</td>\n",
       "      <td>0.846176</td>\n",
       "      <td>0.846460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.430100</td>\n",
       "      <td>0.407818</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.847079</td>\n",
       "      <td>0.847067</td>\n",
       "      <td>0.848090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>0.397668</td>\n",
       "      <td>0.846906</td>\n",
       "      <td>0.846500</td>\n",
       "      <td>0.846128</td>\n",
       "      <td>0.847886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.439900</td>\n",
       "      <td>0.401129</td>\n",
       "      <td>0.847105</td>\n",
       "      <td>0.847407</td>\n",
       "      <td>0.847127</td>\n",
       "      <td>0.847988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.431800</td>\n",
       "      <td>0.405034</td>\n",
       "      <td>0.847020</td>\n",
       "      <td>0.846600</td>\n",
       "      <td>0.846167</td>\n",
       "      <td>0.846460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.431800</td>\n",
       "      <td>0.396861</td>\n",
       "      <td>0.846742</td>\n",
       "      <td>0.846628</td>\n",
       "      <td>0.846308</td>\n",
       "      <td>0.847886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.420800</td>\n",
       "      <td>0.393251</td>\n",
       "      <td>0.849937</td>\n",
       "      <td>0.850244</td>\n",
       "      <td>0.849898</td>\n",
       "      <td>0.850739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.416900</td>\n",
       "      <td>0.395758</td>\n",
       "      <td>0.850374</td>\n",
       "      <td>0.850296</td>\n",
       "      <td>0.850317</td>\n",
       "      <td>0.851248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>0.397797</td>\n",
       "      <td>0.849201</td>\n",
       "      <td>0.849361</td>\n",
       "      <td>0.849248</td>\n",
       "      <td>0.850127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.396741</td>\n",
       "      <td>0.852402</td>\n",
       "      <td>0.852003</td>\n",
       "      <td>0.851820</td>\n",
       "      <td>0.852165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>0.390994</td>\n",
       "      <td>0.853246</td>\n",
       "      <td>0.853043</td>\n",
       "      <td>0.853036</td>\n",
       "      <td>0.853693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.426100</td>\n",
       "      <td>0.386155</td>\n",
       "      <td>0.850460</td>\n",
       "      <td>0.850646</td>\n",
       "      <td>0.850457</td>\n",
       "      <td>0.851146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.422600</td>\n",
       "      <td>0.391462</td>\n",
       "      <td>0.851140</td>\n",
       "      <td>0.851313</td>\n",
       "      <td>0.851191</td>\n",
       "      <td>0.852267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>0.391192</td>\n",
       "      <td>0.850266</td>\n",
       "      <td>0.850447</td>\n",
       "      <td>0.850290</td>\n",
       "      <td>0.851044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.851452</td>\n",
       "      <td>0.851703</td>\n",
       "      <td>0.851529</td>\n",
       "      <td>0.852369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.424400</td>\n",
       "      <td>0.390072</td>\n",
       "      <td>0.850881</td>\n",
       "      <td>0.851099</td>\n",
       "      <td>0.850940</td>\n",
       "      <td>0.851758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.396522</td>\n",
       "      <td>0.851832</td>\n",
       "      <td>0.851979</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>0.851961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.390950</td>\n",
       "      <td>0.851225</td>\n",
       "      <td>0.851458</td>\n",
       "      <td>0.851313</td>\n",
       "      <td>0.852267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.407400</td>\n",
       "      <td>0.395084</td>\n",
       "      <td>0.851821</td>\n",
       "      <td>0.852127</td>\n",
       "      <td>0.851835</td>\n",
       "      <td>0.852573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>0.393325</td>\n",
       "      <td>0.851430</td>\n",
       "      <td>0.851781</td>\n",
       "      <td>0.851433</td>\n",
       "      <td>0.852267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.394765</td>\n",
       "      <td>0.850989</td>\n",
       "      <td>0.851256</td>\n",
       "      <td>0.850871</td>\n",
       "      <td>0.851554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.392860</td>\n",
       "      <td>0.851831</td>\n",
       "      <td>0.852059</td>\n",
       "      <td>0.851849</td>\n",
       "      <td>0.852573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.387189</td>\n",
       "      <td>0.851378</td>\n",
       "      <td>0.851720</td>\n",
       "      <td>0.851452</td>\n",
       "      <td>0.852369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.396800</td>\n",
       "      <td>0.389425</td>\n",
       "      <td>0.851514</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.851566</td>\n",
       "      <td>0.852369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.386543</td>\n",
       "      <td>0.852783</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.852839</td>\n",
       "      <td>0.853693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>0.387961</td>\n",
       "      <td>0.852135</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.852167</td>\n",
       "      <td>0.852980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.406200</td>\n",
       "      <td>0.389272</td>\n",
       "      <td>0.852811</td>\n",
       "      <td>0.853166</td>\n",
       "      <td>0.852862</td>\n",
       "      <td>0.853693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.388276</td>\n",
       "      <td>0.862077</td>\n",
       "      <td>0.862397</td>\n",
       "      <td>0.862152</td>\n",
       "      <td>0.862980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.418900</td>\n",
       "      <td>0.389516</td>\n",
       "      <td>0.851590</td>\n",
       "      <td>0.851894</td>\n",
       "      <td>0.851612</td>\n",
       "      <td>0.852369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.401100</td>\n",
       "      <td>0.389201</td>\n",
       "      <td>0.852623</td>\n",
       "      <td>0.852949</td>\n",
       "      <td>0.852707</td>\n",
       "      <td>0.853591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.412300</td>\n",
       "      <td>0.388973</td>\n",
       "      <td>0.862825</td>\n",
       "      <td>0.863125</td>\n",
       "      <td>0.862896</td>\n",
       "      <td>0.863693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.388863</td>\n",
       "      <td>0.862682</td>\n",
       "      <td>0.862957</td>\n",
       "      <td>0.862734</td>\n",
       "      <td>0.863490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.405500</td>\n",
       "      <td>0.388940</td>\n",
       "      <td>0.862682</td>\n",
       "      <td>0.862957</td>\n",
       "      <td>0.862734</td>\n",
       "      <td>0.863490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.410500</td>\n",
       "      <td>0.388923</td>\n",
       "      <td>0.852574</td>\n",
       "      <td>0.852850</td>\n",
       "      <td>0.852627</td>\n",
       "      <td>0.853388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49088, training_loss=0.47576129172274156, metrics={'train_runtime': 3891.953, 'train_samples_per_second': 201.802, 'train_steps_per_second': 12.613, 'total_flos': 142281665524800.0, 'train_loss': 0.47576129172274156, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f212cb8-1e78-4101-bf03-bc2a62d976c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
