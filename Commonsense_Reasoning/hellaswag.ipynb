{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8947e2c-a06e-4f07-a676-f1ea41f1565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training, AdaLoraConfig, AdaLoraConfig\n",
    "\n",
    "from transformers import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15178bec-c1a7-4c59-8bc0-73a69bd9e8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12335ab7aa9d4f23a21da2426d61cec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/43.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ca0004ae4445cf82e62a3f258c7533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the local JSON file\n",
    "raw_datasets = load_dataset('json', data_files='https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/hellaswag/train.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff6abb6-d878-438a-8252-b648ceca4db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'answer'],\n",
       "        num_rows: 39905\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ef74298-2867-4916-b08b-449abae8133b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please choose the correct ending to complete the given sentence: Painting: . a lady named linda, creator of paint along\\n\\nEnding1: with two older women are shown painting side by side with a picture of santa claus and a few items floating around in the background. Ending2: is demonstrating how to do an acrylic painting. Ending3: with three other girls is standing next to a table talking about the paint. Ending4: with channel 9 and female robots, remakes a process called paintball guns for entire alien cities in asia.\\n\\nAnswer format: ending1/ending2/ending3/ending4'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['instruction'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ffa4932-69e4-4868-b369-00e1545abe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    if data_point[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \n",
    "\n",
    "                ### Instruction:\n",
    "                {data_point[\"instruction\"]}\n",
    "                \n",
    "                ### Input:\n",
    "                {data_point[\"input\"]}\n",
    "                \n",
    "                ### Response:\n",
    "                {data_point[\"output\"]}\"\"\" # noqa: E501\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  \n",
    "\n",
    "                ### Instruction:\n",
    "                {data_point[\"instruction\"]}\n",
    "                \n",
    "                ### Response:\n",
    "                {data_point[\"output\"]}\"\"\" # noqa: E501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d93634b3-de26-45d1-946b-f770058d514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    cutoff_len=1000\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a70247-7768-49bb-8971-47bb63a56b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_inputs=True\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = generate_prompt({**data_point, \"output\": \"\"})\n",
    "        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "                                              -100\n",
    "                                          ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                                                                user_prompt_len:\n",
    "                                                                ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e387ff91-2c51-4337-825b-a732cbd40547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Log in using your Hugging Face token\n",
    "login(\"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\")\n",
    "\n",
    "model_name = \"yahma/llama-7b-hf\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config.hidden_dropout_prob=0.0\n",
    "config.attention_probs_dropout_prob=0.00\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6d9fbe-54e3-4a78-938f-a5beea4bfeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feba83ddce3f4a93a111b03b2ac70a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = raw_datasets[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad153a7-f1c8-45bc-b39a-83fc01688c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7547ec9f-ad18-4053-a57e-9713ef91c833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 39905\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faceac05-23fd-47e6-b4d3-311eec20497f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.  \\n\\n                ### Instruction:\\n                Please choose the correct ending to complete the given sentence: Rollerblading: A woman is seen roller blading down an alley as well as several clips of other people riding around the city. the camera\\n\\nEnding1: continues watching several people ride around and then leads into people in a skate park doing impressive tricks. Ending2: pans around the city while the woman continues to help herself as well as pull obstacles out of the way and begin taking pictures. Ending3: pans all around the area as well as showing one girl playing a piano and talking to the camera and roaming around around. Ending4: pans around the surface of the overpass and pans around to rides going around the city.\\n\\nAnswer format: ending1/ending2/ending3/ending4\\n                \\n                ### Response:\\n                the correct answer is ending1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_data['input_ids'][120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c36a1a-afa5-4e03-8f81-c67fad5aaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5a497dd-196a-4fef-8296-9807c7ed057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8977910b-c948-4208-a6b5-74e3e9620dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faf6499b1d64499ba000e123d9b1d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f275066c-3a5f-42d4-b1bb-5ed2f24559a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leader\n",
    "\n",
    "leader.PEFT(model, method='column', rank=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8435c6b-7f02-404b-8d6d-1962f2e81451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): column()\n",
       "          (k_proj): column()\n",
       "          (v_proj): column()\n",
       "          (o_proj): column()\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): column()\n",
       "          (up_proj): column()\n",
       "          (down_proj): column()\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): column()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f2ae80e-7a76-49dc-8a39-e16c9a67280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters:4441856\n"
     ]
    }
   ],
   "source": [
    "# Count of trainable parameters\n",
    "total_trainable_params = 0\n",
    "total =  0\n",
    "# Print trainable parameters and count their total number\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        #print(f\"Parameter name: {name}, Shape: {param.shape}\")\n",
    "        \n",
    "        total_trainable_params += param.numel()\n",
    "    total+=param.numel()\n",
    "\n",
    "print(f\"Total trainable parameters:{total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b0f8d1d-2b84-477d-b5a0-4f757d49f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f291c573-2dd7-4196-b130-d6e9289fc7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f5fd3f8-fd96-4fde-8d34-1ca8268e3f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4434' max='4434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4434/4434 6:40:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.359800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.362300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.344800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.346300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.354600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.329600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.327700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.336800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.344100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4434, training_loss=1.3532846499844398, metrics={'train_runtime': 24025.1134, 'train_samples_per_second': 1.661, 'train_steps_per_second': 0.185, 'total_flos': 376975628120064.0, 'train_loss': 1.3532846499844398, 'epoch': 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='qnli_dir',\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.00,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    gradient_accumulation_steps= 3,\n",
    "\n",
    "    logging_steps=100,\n",
    "   \n",
    "    #load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    #eval_dataset=tokenized_datasets[\"validation\"],\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d62ff1f5-60ed-4e61-b5b6-71158a37b155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e907e376fd74c6a849701c45b28e49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94dc20350984186bcd94c01f05d9351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_datasets = load_dataset('json', data_files='https://raw.githubusercontent.com/AGI-Edgerunners/LLM-Adapters/refs/heads/main/dataset/hellaswag/test.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66e8579a-32bc-4416-b680-cad2fd0d2c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'answer'],\n",
       "        num_rows: 10042\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "912fb79a-8eb0-4856-84bc-899000925332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ceef90599804e9b932330b156b4665d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = test_datasets[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "292272a9-c7f2-4b0c-9a3c-86e540af7fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10042\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a95aff6-ea67-4180-a673-a3caf3b945cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.  \\n\\n                ### Instruction:\\n                Please choose the correct ending to complete the given sentence: High jump: A man is preparing to run and jump. he\\n\\nEnding1: runs very quickly down the track. Ending2: is doing a stunt, then dismounts to the ground. Ending3: releases the bar, and lands a few feet away. Ending4: is blindfolded and jumps full speed into the sand.\\n\\nAnswer format: ending1/ending2/ending3/ending4\\n                \\n                ### Response:\\n                the correct answer is'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test_data['input_ids'][18][:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1408ca1b-4fdf-4880-9b94-be9031a93467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85da30422114bfb9b6973269c2d71f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f286cb4e440>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "k = 0\n",
    "ck = 0\n",
    "result = []\n",
    "truth = []\n",
    "for i in tqdm(range(len(test_data['input_ids']))):\n",
    "    input_truth = torch.tensor(test_data['input_ids'][i])\n",
    "    input_ids = torch.tensor(test_data['input_ids'][i][:-2]).unsqueeze(0).to(model.device)\n",
    "    attention_mask = torch.tensor(test_data['attention_mask'][i][:-2]).unsqueeze(0).to(model.device)\n",
    "    #print(input_ids.shape, attention_mask.shape)\n",
    "    output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=input_ids.shape[1]+7)\n",
    "\n",
    "    try:\n",
    "        output = tokenizer.decode(output[0]).split('correct answer')[1]\n",
    "        true = tokenizer.decode(input_truth).split('correct answer')[1]\n",
    "        #print(output, true)\n",
    "\n",
    "        result.append(output)\n",
    "        truth.append(true)\n",
    "       \n",
    "\n",
    "    except:\n",
    "        ck = 1\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46aee209-4486-43b6-85ef-490597459259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending1',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending1',\n",
       " ' is ending3',\n",
       " ' is ending1',\n",
       " ' is ending4',\n",
       " ' is ending4',\n",
       " ' is ending2',\n",
       " ' is ending3',\n",
       " ' is ending2',\n",
       " ' is ending4',\n",
       " ' is ending3',\n",
       " ' is ending4',\n",
       " ' is ending3']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "504cbaed-49df-45e8-9bdb-78b7fa32c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_result1 = []\n",
    "for i in result:\n",
    "    if 'ending1' in i:\n",
    "        cleaned_result1.append('ending1')\n",
    "    elif 'ending2' in i:\n",
    "        cleaned_result1.append('ending2')\n",
    "    elif 'ending3' in i:\n",
    "        cleaned_result1.append('ending3')\n",
    "    elif 'ending4' in i:\n",
    "        cleaned_result1.append('ending4')        \n",
    "        \n",
    "cleaned_truth1 = []\n",
    "for i in truth:\n",
    "    if 'ending1' in i:\n",
    "        cleaned_truth1.append('ending1')\n",
    "    elif 'ending2' in i:\n",
    "        cleaned_truth1.append('ending2')\n",
    "    elif 'ending3' in i:\n",
    "        cleaned_truth1.append('ending3')\n",
    "    elif 'ending4' in i:\n",
    "        cleaned_truth1.append('ending4')        \n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cffa91e4-a612-4c7a-bb35-7f3dcc136fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8945578231292517\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "corr = 0\n",
    "incorr = 0\n",
    "for i, j in zip(cleaned_truth1, cleaned_result1):\n",
    "    if i == j:\n",
    "        corr+=1\n",
    "    else:\n",
    "        incorr+=1\n",
    "      \n",
    "\n",
    "print('Accuracy:', corr/(corr+incorr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f61039-0171-48ce-ba1e-ba19759c445d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
