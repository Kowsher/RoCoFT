{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4b5a4c-e5be-4e99-acbc-a59c5d98efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertForSequenceClassification\n",
    "from functorch import jacrev, make_functional_with_buffers\n",
    "import gc\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "from torch.autograd import grad\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets  = load_dataset(\"glue\", 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1e8143-4539-4958-8b3d-a60eedaa03b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "\n",
    "#config.num_labels=2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b3882b-fa4e-4787-83bf-61ddf1ee59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocessing_function, batched=True)\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b747c81f-3429-4b40-a099-0db0d42cda68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers.activations import ACT2FN\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84403687-6c7b-4b0a-bc6f-6da88ea8acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leader\n",
    "\n",
    "leader.PEFT(model, method='row', targets=['key'], rank=1) \n",
    "#targets=['key', 'value', 'dense', 'query'])\n",
    "# method = 'row', 'column', 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ac7d06-c386-478e-a459-797f2adafbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1ab54a-85e1-4d55-a32c-12df9a5e587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dir',\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.00,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=100,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa2b274-91c3-40d6-a981-ba0fcccf5e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"it 's a charming and often affecting journey . \",\n",
       " 'unflinchingly bleak and desperate ',\n",
       " 'allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . ',\n",
       " \"the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \",\n",
       " \"it 's slow -- very , very slow . \",\n",
       " 'although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . ',\n",
       " 'a sometimes tedious film . ',\n",
       " \"or doing last year 's taxes with your ex-wife . \",\n",
       " \"you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance . \",\n",
       " \"in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey . \"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"validation\"]['sentence'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a56645-8eb7-4d40-89d0-2017ebaa6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8326dbeb-6031-4f69-968a-b52eaa42d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze specific layers by name\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86a7c1f-ba56-49c7-ab44-ed04e3adb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize input text\n",
    "def tokenize_input(texts, tokenizer, max_length=10):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Example input sets\n",
    "texts_set_1 = tokenized_datasets[\"validation\"]['sentence'][0:5]\n",
    "texts_set_2 = tokenized_datasets[\"validation\"]['sentence'][5:8]\n",
    "\n",
    "\n",
    "# Tokenize inputs\n",
    "input_set_1 = tokenize_input(texts_set_1, tokenizer)\n",
    "input_set_2 = tokenize_input(texts_set_2, tokenizer)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "input_set_1 = {k: v.to(device) for k, v in input_set_1.items()}\n",
    "input_set_2 = {k: v.to(device) for k, v in input_set_2.items()}\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ddab6d-ea91-4cad-b181-16da61a1af8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/.local/lib/python3.10/site-packages/torch/_functorch/deprecated.py:101: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional_with_buffers is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('make_functional_with_buffers', 'torch.func.functional_call')\n",
      "/home/kowsher/.local/lib/python3.10/site-packages/torch/_functorch/deprecated.py:80: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.jacrev is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.jacrev instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('jacrev')\n",
      "/home/kowsher/.local/lib/python3.10/site-packages/torch/_functorch/deprecated.py:58: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch.nn as nn\n",
    "from functorch import make_functional_with_buffers, vmap, jacrev\n",
    "\n",
    "\n",
    "# Get word embeddings instead of text input\n",
    "def get_word_embeddings(input_ids):\n",
    "    with torch.no_grad():\n",
    "        outputs = model.roberta.embeddings(input_ids)\n",
    "    return outputs\n",
    "\n",
    "# We will focus on the last layer output\n",
    "class BertLastLayer(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertLastLayer, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        # Get the last hidden states from BERT\n",
    "        outputs = self.bert_model(inputs_embeds=embeddings)\n",
    "        # We are only interested in the last hidden state\n",
    "        last_hidden_state = outputs.logits\n",
    "        return last_hidden_state  # CLS token's representation for simplicity\n",
    "\n",
    "# Example usage:\n",
    "device = 'cuda'\n",
    "bert_last_layer = BertLastLayer(model).to(device)\n",
    "\n",
    "# Example sentence\n",
    "sentence = [\"hello\", 'world']\n",
    "\n",
    "# Tokenize the sentence\n",
    "#inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "input_ids_train = input_set_1['input_ids']\n",
    "\n",
    "# Get word embeddings for the sentence\n",
    "x_train = get_word_embeddings(input_ids_train)\n",
    "\n",
    "input_ids_test = input_set_2['input_ids']\n",
    "\n",
    "# Get word embeddings for the sentence\n",
    "x_test = get_word_embeddings(input_ids_test)\n",
    "\n",
    "# Convert the BERT model to a functional model using functorch, including buffers\n",
    "fnet, params, buffers = make_functional_with_buffers(bert_last_layer)\n",
    "\n",
    "# Function for a single pass through the functional model\n",
    "def fnet_single(params, buffers, x):\n",
    "    return fnet(params, buffers, x.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# NTK Calculation (similar to your original code)\n",
    "def empirical_ntk_jacobian_contraction(fnet_single, params, buffers, x1, x2):\n",
    "    # Compute J(x1)\n",
    "    jac1 = vmap(jacrev(fnet_single), (None, None, 0))(params, buffers, x1)\n",
    "    jac1 = [j.flatten(2) for j in jac1]\n",
    "\n",
    "    # Compute J(x2)\n",
    "    jac2 = vmap(jacrev(fnet_single), (None, None, 0))(params, buffers, x2)\n",
    "    jac2 = [j.flatten(2) for j in jac2]\n",
    "\n",
    "    # Compute J(x1) @ J(x2).T\n",
    "    result = torch.stack([torch.einsum('Naf,Mbf->NMab', j1, j2) for j1, j2 in zip(jac1, jac2)])\n",
    "    result = result.sum(0)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Compute NTK\n",
    "result = empirical_ntk_jacobian_contraction(fnet_single, params, buffers, x_train, x_test)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0cfa2f-b908-4b0a-8c37-e2be8954e38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
